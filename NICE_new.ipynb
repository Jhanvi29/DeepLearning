{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VTccuoEbzSBH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Coupling(nn.Module):\n",
        "    def __init__(self, in_out_dim, mid_dim, hidden, mask_config):\n",
        "        \"\"\"Initialize a coupling layer.\n",
        "        Args:\n",
        "            in_out_dim: input/output dimensions.\n",
        "            mid_dim: number of units in a hidden layer.\n",
        "            hidden: number of hidden layers.\n",
        "            mask_config: 1 if transform odd units, 0 if transform even units.\n",
        "        \"\"\"\n",
        "        super(Coupling, self).__init__()\n",
        "        self.mask_config = mask_config\n",
        "\n",
        "        self.in_block = nn.Sequential(\n",
        "            nn.Linear(in_out_dim//2, mid_dim),\n",
        "            nn.ReLU())\n",
        "        self.mid_block = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(mid_dim, mid_dim),\n",
        "                nn.ReLU()) for _ in range(hidden - 1)])\n",
        "        self.out_block = nn.Linear(mid_dim, in_out_dim//2)\n",
        "\n",
        "    def forward(self, x, reverse=False):\n",
        "        \"\"\"Forward pass.\n",
        "        Args:\n",
        "            x: input tensor.\n",
        "            reverse: True in inference mode, False in sampling mode.\n",
        "        Returns:\n",
        "            transformed tensor.\n",
        "        \"\"\"\n",
        "        [B, W] = list(x.size())\n",
        "        x = x.reshape((B, W//2, 2))\n",
        "        if self.mask_config:\n",
        "            on, off = x[:, :, 0], x[:, :, 1]\n",
        "        else:\n",
        "            off, on = x[:, :, 0], x[:, :, 1]\n",
        "\n",
        "        off_ = self.in_block(off)\n",
        "        for i in range(len(self.mid_block)):\n",
        "            off_ = self.mid_block[i](off_)\n",
        "        shift = self.out_block(off_)\n",
        "        if reverse:\n",
        "            on = on - shift\n",
        "        else:\n",
        "            on = on + shift\n",
        "\n",
        "        if self.mask_config:\n",
        "            x = torch.stack((on, off), dim=2)\n",
        "        else:\n",
        "            x = torch.stack((off, on), dim=2)\n",
        "        return x.reshape((B, W))\n",
        "\n",
        "\"\"\"Log-scaling layer.\n",
        "\"\"\"\n",
        "class Scaling(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        \"\"\"Initialize a (log-)scaling layer.\n",
        "        Args:\n",
        "            dim: input/output dimensions.\n",
        "        \"\"\"\n",
        "        super(Scaling, self).__init__()\n",
        "        self.scale = nn.Parameter(\n",
        "            torch.zeros((1, dim)), requires_grad=True)\n",
        "\n",
        "    def forward(self, x, reverse=False):\n",
        "        \"\"\"Forward pass.\n",
        "        Args:\n",
        "            x: input tensor.\n",
        "            reverse: True in inference mode, False in sampling mode.\n",
        "        Returns:\n",
        "            transformed tensor and log-determinant of Jacobian.\n",
        "        \"\"\"\n",
        "        log_det_J = torch.sum(self.scale)\n",
        "        if reverse:\n",
        "            x = x * torch.exp(-self.scale)\n",
        "        else:\n",
        "            x = x * torch.exp(self.scale)\n",
        "        return x, log_det_J\n",
        "\n",
        "\"\"\"NICE main model.\n",
        "\"\"\"\n",
        "class NICE(nn.Module):\n",
        "    def __init__(self, prior, coupling, \n",
        "        in_out_dim, mid_dim, hidden, mask_config):\n",
        "        \"\"\"Initialize a NICE.\n",
        "        Args:\n",
        "            prior: prior distribution over latent space Z.\n",
        "            coupling: number of coupling layers.\n",
        "            in_out_dim: input/output dimensions.\n",
        "            mid_dim: number of units in a hidden layer.\n",
        "            hidden: number of hidden layers.\n",
        "            mask_config: 1 if transform odd units, 0 if transform even units.\n",
        "        \"\"\"\n",
        "        super(NICE, self).__init__()\n",
        "        self.prior = prior\n",
        "        self.in_out_dim = in_out_dim\n",
        "\n",
        "        self.coupling = nn.ModuleList([\n",
        "            Coupling(in_out_dim=in_out_dim, \n",
        "                     mid_dim=mid_dim, \n",
        "                     hidden=hidden, \n",
        "                     mask_config=(mask_config+i)%2) \\\n",
        "            for i in range(coupling)])\n",
        "        self.scaling = Scaling(in_out_dim)\n",
        "\n",
        "    def g(self, z):\n",
        "        \"\"\"Transformation g: Z -> X (inverse of f).\n",
        "        Args:\n",
        "            z: tensor in latent space Z.\n",
        "        Returns:\n",
        "            transformed tensor in data space X.\n",
        "        \"\"\"\n",
        "        x, _ = self.scaling(z, reverse=True)\n",
        "        for i in reversed(range(len(self.coupling))):\n",
        "            x  = self.coupling[i](x, reverse=True)\n",
        "        return x \n",
        "\n",
        "    def f(self, x):\n",
        "        \"\"\"Transformation f: X -> Z (inverse of g).\n",
        "        Args:\n",
        "            x: tensor in data space X.\n",
        "        Returns:\n",
        "            transformed tensor in latent space Z.\n",
        "        \"\"\"\n",
        "        for i in range(len(self.coupling)):\n",
        "            x = self.coupling[i](x)\n",
        "        return self.scaling(x)\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        \"\"\"Computes data log-likelihood.\n",
        "        (See Section 3.3 in the NICE paper.)\n",
        "        Args:\n",
        "            x: input minibatch.\n",
        "        Returns:\n",
        "            log-likelihood of input.\n",
        "        \"\"\"\n",
        "        z, log_det_J = self.f(x)\n",
        "        log_ll = torch.sum(self.prior.log_prob(z), dim=1)\n",
        "        return log_ll + log_det_J\n",
        "\n",
        "    def sample(self, size):\n",
        "        \"\"\"Generates samples.\n",
        "        Args:\n",
        "            size: number of samples to generate.\n",
        "        Returns:\n",
        "            samples from the data space X.\n",
        "        \"\"\"\n",
        "        z = self.prior.sample((size, self.in_out_dim)).cuda()\n",
        "        return self.g(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "        Args:\n",
        "            x: input minibatch.\n",
        "        Returns:\n",
        "            log-likelihood of input.\n",
        "        \"\"\"\n",
        "        return self.log_prob(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k63g1XYh0dog",
        "outputId": "90fdde8d-e1bc-48df-c791-1731afacf939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class '__main__.StandardLogistic'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  'with `validate_args=False` to turn off validation.')\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "class StandardLogistic(torch.distributions.Distribution):\n",
        "    def __init__(self):\n",
        "        super(StandardLogistic, self).__init__()\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        \"\"\"Computes data log-likelihood.\n",
        "        Args:\n",
        "            x: input tensor.\n",
        "        Returns:\n",
        "            log-likelihood.\n",
        "        \"\"\"\n",
        "        return -(F.softplus(x) + F.softplus(-x))\n",
        "\n",
        "    def sample(self, size):\n",
        "        \"\"\"Samples from the distribution.\n",
        "        Args:\n",
        "            size: number of samples to generate.\n",
        "        Returns:\n",
        "            samples.\n",
        "        \"\"\"\n",
        "        z = torch.distributions.Uniform(0., 1.).sample(size).cuda()\n",
        "        return torch.log(z) - torch.log(1. - z)\n",
        "\n",
        "prior = StandardLogistic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sfnKyRFq07mM"
      },
      "outputs": [],
      "source": [
        "device = 'cuda:0'\n",
        "flow = NICE(prior=prior, \n",
        "                coupling=4, \n",
        "                in_out_dim=10, \n",
        "                mid_dim=10, \n",
        "                hidden=4, \n",
        "                mask_config=1).to(device)\n",
        "optimizer = torch.optim.Adam(\n",
        "        flow.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6vD8TToH2MEa"
      },
      "outputs": [],
      "source": [
        "z_iso = torch.randn(10,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lSf-yXlJ2RC5"
      },
      "outputs": [],
      "source": [
        "running_loss = 0\n",
        "for i in range (100):\n",
        "  flow.train()\n",
        "  optimizer.zero_grad()\n",
        "  loss = -flow(z_iso.to(device)).mean()\n",
        "  running_loss += float(loss)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1ABYJaS4nJd",
        "outputId": "beb6d1ce-ec0f-4547-b12c-e2146684c58c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.0117, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "torch.Size([100, 1])\n"
          ]
        }
      ],
      "source": [
        "z_iso_latent , det  = flow.f(z_iso.to(device))\n",
        "print(det)\n",
        "z_iso_new = z_iso_latent.view(100 , 1)\n",
        "print(z_iso_new.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tIQ09i9W5h-g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch as t\n",
        "num_dims = 100\n",
        "diag_precisions = np.linspace(1., 1000., num_dims)**-1\n",
        "\n",
        "q, _ = np.linalg.qr(t.randn(num_dims, num_dims))\n",
        "scg_prec = (q * diag_precisions).dot(q.T)\n",
        "scg_prec = scg_prec.astype(np.float32)\n",
        "scg_var = np.linalg.inv(scg_prec) / 1000.0\n",
        "\n",
        "covariance = t.tensor(scg_var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qjSvNMeI6Ij-"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from numpy import *\n",
        "def norm_pdf_multivariate(x, mu, sigma):\n",
        "    size = len(x)\n",
        "    if size == len(mu) and (size, size) == sigma.shape:\n",
        "        det = t.linalg.det(sigma)\n",
        "        norm_const = 1.0/ ( math.pow((2*pi),float(size)/2) * math.pow(det,1.0/2) )\n",
        "        x_mu = (x - mu)\n",
        "        inv = t.inverse(sigma)       \n",
        "        mul = t.exp (-0.5 * (x_mu * inv * x_mu.T))\n",
        "        return norm_const , mul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wYlC-hE_6L0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52cd070e-87a4-4d20-9c0e-373ff930b05d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1])\n",
            "torch.Size([100, 100])\n",
            "torch.Size([100, 1])\n"
          ]
        }
      ],
      "source": [
        "mu = torch.ones(100,1)\n",
        "print(mu.shape)\n",
        "print(covariance.shape)\n",
        "print(z_iso_new.shape)\n",
        "\n",
        "norm , target = norm_pdf_multivariate(z_iso_new , mu.to(device) , covariance.to(device))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}